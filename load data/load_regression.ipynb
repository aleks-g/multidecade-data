{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-FileCopyrightText: 2022 Aleksander Grochowicz\n",
    "#\n",
    "# SPDX-License-Identifier: GPL-3.0-or-later\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import holidays\n",
    "import tabulate\n",
    "from utilities import compute_correlation, compute_rmse, load_regression, compute_cdd_hdd, daily_regression, create_daily_data, create_hourly_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiation\n",
    "## Input data:\n",
    "- Load data for Europe [from 2006 to 2015, hourly, country-level](europe_demand_2006-2015.csv) based on [ENTSO-E data](../original%20data/europe_demand_2006-2015_original.csv) (where we use [this data from Swissgrid](../original%20data/CH_demand_2010-2015.csv) for 2010 to 2015).\n",
    "- Temperature data for Europe [from 1980 to 2020, hourly, country-level](europe_temperatures_1980-2020.csv) from ERA5 reanalysis, aggregated by Atlite to hourly values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose years to use for training and test data.\n",
    "startyear = 2010\n",
    "endyear = 2014 + 1  # 2010-2014 inclusive.\n",
    "val_year = 2015\n",
    "#region = 'europe'\n",
    "years = list(range(startyear, endyear))\n",
    "nb_y = len(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load demand data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overwrite Swiss data\n",
    "The ENTSO-E data for Switzerland is flawed, so we replace it with data from Swissgrid. The next cell corrects the Swiss ENTSO-E data with data from Swissgrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitute faulty demand for 'CH' with data from Swissgrid.\n",
    "# https://www.swissgrid.ch/en/home/operation/grid-data/generation.html#total-energy-consumption\n",
    "# Energy Statistic for years 2010 to 2015 > Zeitreihen0h15, 'Summe verbrauchte Energie Regelblock Schweiz / Total energy consumption Swiss controlblock'\n",
    "\n",
    "demand = pd.read_csv('original_data/europe_demand_2006-2015_original.csv', sep = ',', index_col = 0)\n",
    "demand = demand.rename(columns={\"GB\": \"UK\", \"GR\": \"EL\"}) \n",
    "demand_CH = pd.read_csv('original_data/CH_demand_2010-2015.csv', skiprows=3, index_col = 0)\n",
    "demand_CH.index = pd.to_datetime(demand_CH.index)\n",
    "demand_CH = demand_CH.resample('1H').sum()\n",
    "demand_CH = demand_CH.iloc[:-1]\n",
    "demand_CH = round((demand_CH / 10**3), 0)\n",
    "\n",
    "dem_CH = pd.DataFrame()\n",
    "dem_CH.index = pd.date_range(start = date(2006, 1,1), end = date(2016, 1, 1), freq = '1H')\n",
    "dem_CH = dem_CH.join(demand_CH)\n",
    "demand['CH'] = dem_CH['kWh'].values[:-1]\n",
    "demand = round(demand, 0)\n",
    "demand.to_csv('processing/europe_demand_2006-2015_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "demand = pd.read_csv('processing/europe_demand_2006-2015_corrected.csv', index_col = 0, infer_datetime_format=True)\n",
    "demand.index = pd.to_datetime(demand.index)\n",
    "\n",
    "# Test data\n",
    "demand_val = pd.read_csv('processing/europe_demand_2006-2015_corrected.csv', index_col = 0, infer_datetime_format=True) #for Greece the last hour in 2015 just got the value of the penultimate\n",
    "demand_val.index = pd.to_datetime(demand_val.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load temperature data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv('original_data/europe_temperatures_2010-2015.csv', index_col = [0,2], infer_datetime_format=True)\n",
    "temp = {} #create dictionary for each country\n",
    "for j in t.index.levels[0]:\n",
    "    temp[j] = t.loc[j]\n",
    "    temp[j].index = pd.to_datetime(temp[j].index)\n",
    "    temp[j] = temp[j]\n",
    "\n",
    "# Same preparation for validation data\n",
    "t_val = pd.read_csv('original_data/europe_temperatures_2010-2015.csv', index_col = [0,2], infer_datetime_format=True)\n",
    "t_val = t_val[['temp']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick countries to study\n",
    "Some countries are currently excluded: Iceland, Cyprus, Malta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AT', 'BA', 'BE', 'BG', 'CH', 'CS', 'CY', 'CZ', 'DE', 'DK', 'DK_W',\n",
       "       'EE', 'ES', 'FI', 'FR', 'UK', 'EL', 'HR', 'HU', 'IE', 'IS', 'IT', 'LT',\n",
       "       'LU', 'LV', 'ME', 'MK', 'NI', 'NL', 'NO', 'PL', 'PT', 'RO', 'RS', 'SE',\n",
       "       'SI', 'SK', 'UA_W', 'MT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demand.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = demand.columns\n",
    "c2 = t.index.levels[0]\n",
    "all_countries = c1.intersection(c2)\n",
    "countries = list(all_countries)\n",
    "countries.remove('IS') \n",
    "countries.remove('CY') \n",
    "countries.remove('MT')\n",
    "nb_c = len(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with demand time series for all countries\n",
    "d = {}\n",
    "for i in countries:\n",
    "    d[\"{0}\".format(i)] = pd.DataFrame(demand[i].loc[str(startyear):str(endyear - 1)].copy())\n",
    "\n",
    "# Determine day of the week  \n",
    "for i in d.keys():\n",
    "    d[i] = pd.DataFrame(d[i])\n",
    "    d[i]['weekday'] = d[i].index.to_series().dt.dayofweek\n",
    "    d[i].columns = ['demand', 'weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for AT\n",
      "                first                last  count\n",
      "0 2006-03-26 02:00:00 2006-03-26 02:00:00      1\n",
      "1 2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2 2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3 2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "4 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "5 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "7 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "8 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "9 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for BE\n",
      "                first                last  count\n",
      "0 2006-03-26 02:00:00 2006-03-26 02:00:00      1\n",
      "1 2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2 2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3 2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "4 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "5 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "7 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "8 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "9 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for BG\n",
      "                first                last  count\n",
      "0 2006-03-26 02:00:00 2006-03-26 02:00:00      1\n",
      "1 2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2 2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3 2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "4 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "5 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "7 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "8 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "9 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for CH\n",
      "       first                last  count\n",
      "0 2006-01-01 2009-12-31 23:00:00  35064\n",
      "Missing values for CZ\n",
      "                first                last  count\n",
      "0 2006-03-26 02:00:00 2006-03-26 02:00:00      1\n",
      "1 2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2 2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3 2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "4 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "5 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "7 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "8 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "9 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for DE\n",
      "                first                last  count\n",
      "0 2006-03-26 02:00:00 2006-03-26 02:00:00      1\n",
      "1 2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2 2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3 2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "4 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "5 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "7 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "8 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "9 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for DK\n",
      "                first                last  count\n",
      "0 2006-01-01 00:00:00 2009-12-31 23:00:00  35064\n",
      "1 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "2 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "3 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "4 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "5 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "Missing values for EE\n",
      "                first                last  count\n",
      "0 2006-01-01 00:00:00 2008-12-31 23:00:00  26304\n",
      "1 2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "2 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "3 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "4 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "5 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "6 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "7 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "8 2015-06-26 04:00:00 2015-06-26 05:00:00      2\n",
      "9 2015-07-23 03:00:00 2015-07-23 06:00:00      4\n",
      "Missing values for ES\n",
      "                first                last  count\n",
      "0 2006-03-26 02:00:00 2006-03-26 02:00:00      1\n",
      "1 2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2 2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3 2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "4 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "5 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "7 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "8 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "9 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for FI\n",
      "                first                last  count\n",
      "0 2006-01-01 00:00:00 2009-12-31 23:00:00  35064\n",
      "1 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "2 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "3 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "4 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "5 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "6 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for FR\n",
      "                first                last  count\n",
      "0 2006-03-26 02:00:00 2006-03-26 02:00:00      1\n",
      "1 2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2 2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3 2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "4 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "5 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "7 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "8 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "Missing values for UK\n",
      "                first                last  count\n",
      "0 2006-01-01 00:00:00 2009-12-31 23:00:00  35064\n",
      "1 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "2 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "3 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "4 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "5 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "6 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for EL\n",
      "                 first                last  count\n",
      "0  2006-03-26 02:00:00 2006-03-26 02:00:00      1\n",
      "1  2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2  2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3  2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "4  2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "5  2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6  2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "7  2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "8  2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "9  2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "10 2015-04-30 23:00:00 2015-04-30 23:00:00      1\n",
      "11 2015-05-31 23:00:00 2015-05-31 23:00:00      1\n",
      "12 2015-06-30 23:00:00 2015-06-30 23:00:00      1\n",
      "13 2015-07-31 23:00:00 2015-07-31 23:00:00      1\n",
      "14 2015-08-11 23:00:00 2015-08-15 22:00:00     96\n",
      "15 2015-08-31 23:00:00 2015-08-31 23:00:00      1\n",
      "16 2015-09-30 23:00:00 2015-09-30 23:00:00      1\n",
      "17 2015-10-31 23:00:00 2015-10-31 23:00:00      1\n",
      "18 2015-11-30 23:00:00 2015-11-30 23:00:00      1\n",
      "19 2015-12-31 23:00:00 2015-12-31 23:00:00      1\n",
      "Missing values for HR\n",
      "                first                last  count\n",
      "0 2006-03-26 02:00:00 2006-03-26 02:00:00      1\n",
      "1 2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2 2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3 2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "4 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "5 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "7 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "8 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "9 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for HU\n",
      "                first                last  count\n",
      "0 2006-03-26 02:00:00 2006-03-26 02:00:00      1\n",
      "1 2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2 2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3 2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "4 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "5 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "7 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "8 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "9 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for IE\n",
      "                first                last  count\n",
      "0 2006-01-01 00:00:00 2007-12-31 23:00:00  17520\n",
      "1 2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "2 2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "3 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "4 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "5 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "6 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "7 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "8 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for IT\n",
      "                first                last  count\n",
      "0 2006-03-26 02:00:00 2006-03-26 02:00:00      1\n",
      "1 2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2 2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3 2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "4 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "5 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "7 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "8 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "9 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for LT\n",
      "                first                last  count\n",
      "0 2006-01-01 00:00:00 2009-12-31 23:00:00  35064\n",
      "1 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "2 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "3 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "4 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "5 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "6 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for LU\n",
      "                first                last  count\n",
      "0 2006-03-26 02:00:00 2006-03-26 02:00:00      1\n",
      "1 2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2 2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3 2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "4 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "5 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "7 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "8 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "9 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for LV\n",
      "                first                last  count\n",
      "0 2006-01-01 00:00:00 2009-12-31 23:00:00  35064\n",
      "1 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "2 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "3 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "4 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "5 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "6 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for ME\n",
      "                 first                last  count\n",
      "0  2006-01-01 00:00:00 2006-12-31 23:00:00   8760\n",
      "1  2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2  2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3  2009-03-01 00:00:00 2009-03-31 23:00:00    744\n",
      "4  2009-10-01 00:00:00 2009-12-31 23:00:00   2208\n",
      "5  2010-03-28 01:00:00 2010-03-28 02:00:00      2\n",
      "6  2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "7  2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "8  2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "9  2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "10 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for MK\n",
      "                first                last  count\n",
      "0 2006-03-26 02:00:00 2006-03-26 02:00:00      1\n",
      "1 2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2 2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3 2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "4 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "5 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6 2012-03-25 02:00:00 2012-03-25 03:00:00      2\n",
      "7 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "8 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "9 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for NL\n",
      "                first                last  count\n",
      "0 2006-03-26 02:00:00 2006-03-26 02:00:00      1\n",
      "1 2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2 2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3 2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "4 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "5 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "7 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "8 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "9 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for NO\n",
      "                first                last  count\n",
      "0 2006-01-01 00:00:00 2009-12-31 23:00:00  35064\n",
      "1 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "2 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "3 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "4 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "5 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "6 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for PL\n",
      "                first                last  count\n",
      "0 2006-03-26 02:00:00 2006-03-26 02:00:00      1\n",
      "1 2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2 2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3 2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "4 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "5 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "7 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "8 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "9 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for PT\n",
      "                first                last  count\n",
      "0 2006-03-26 02:00:00 2006-03-26 02:00:00      1\n",
      "1 2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2 2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3 2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "4 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "5 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "7 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "8 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "9 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for RO\n",
      "                first                last  count\n",
      "0 2006-03-26 02:00:00 2006-03-26 02:00:00      1\n",
      "1 2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2 2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3 2009-03-29 02:00:00 2009-03-29 03:00:00      2\n",
      "4 2010-03-28 02:00:00 2010-03-28 03:00:00      2\n",
      "5 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "7 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "8 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "9 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for RS\n",
      "                first                last  count\n",
      "0 2006-01-01 00:00:00 2006-12-31 23:00:00   8760\n",
      "1 2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2 2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3 2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "4 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "5 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "7 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "8 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "9 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for SE\n",
      "                first                last  count\n",
      "0 2006-01-01 00:00:00 2009-12-31 23:00:00  35064\n",
      "1 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "2 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "3 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "4 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "5 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "6 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for SI\n",
      "                first                last  count\n",
      "0 2006-03-26 02:00:00 2006-03-26 02:00:00      1\n",
      "1 2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2 2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3 2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "4 2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "5 2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6 2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "7 2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "8 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "9 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n",
      "Missing values for SK\n",
      "                 first                last  count\n",
      "0  2006-03-26 02:00:00 2006-03-26 02:00:00      1\n",
      "1  2007-03-25 02:00:00 2007-03-25 02:00:00      1\n",
      "2  2008-03-30 02:00:00 2008-03-30 02:00:00      1\n",
      "3  2009-03-29 02:00:00 2009-03-29 02:00:00      1\n",
      "4  2010-03-28 02:00:00 2010-03-28 02:00:00      1\n",
      "5  2011-03-27 02:00:00 2011-03-27 02:00:00      1\n",
      "6  2012-03-25 02:00:00 2012-03-25 02:00:00      1\n",
      "7  2012-03-25 23:00:00 2012-03-25 23:00:00      1\n",
      "8  2013-03-31 02:00:00 2013-03-31 02:00:00      1\n",
      "9  2013-03-31 23:00:00 2013-03-31 23:00:00      1\n",
      "10 2014-03-30 02:00:00 2014-03-30 02:00:00      1\n",
      "11 2015-03-29 02:00:00 2015-03-29 02:00:00      1\n"
     ]
    }
   ],
   "source": [
    "# For chosen countries check for periods of NaN values: https://stackoverflow.com/questions/66426653/count-contiguous-nan-values-and-get-the-start-and-final-date-of-the-nan-contiguo\n",
    "# This should show missing values in all years, including the validation year.\n",
    "for c in countries:\n",
    "    s = demand.index.to_series()\n",
    "    m2 = demand[c].isna() #creates masks with difference between days is a day, and where the country's demand contains NaN values\n",
    "    out = s[m2].groupby((~m2).cumsum())\\\n",
    "        .agg(['first', 'last', 'count']).reset_index(drop=True) #aggregate length of periods of missing values first and then group them by the first and last daye as well as the length.\n",
    "    print('Missing values for',c)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with demand data frames for each country\n",
    "d_val = {}\n",
    "for i in countries:\n",
    "    d_val[\"{0}\".format(i)] = pd.DataFrame(demand_val[i].loc[str(val_year)].copy())\n",
    "\n",
    "# Determine day of the week\n",
    "for i in d.keys():\n",
    "    d_val[i] = pd.DataFrame(d_val[i])\n",
    "    d_val[i] = d_val[i].fillna(method = 'bfill')\n",
    "    d_val[i]['weekday'] = d_val[i].index.to_series().dt.dayofweek\n",
    "    d_val[i].columns = ['demand', 'weekday']\n",
    "\n",
    "# Dictionary with temperature for each country \n",
    "temp_val = {} #create dictionary for each country\n",
    "for j in countries:\n",
    "    temp_val[j] = t_val.loc[j]\n",
    "    temp_val[j].index = pd.to_datetime(temp_val[j].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add holidays\n",
    "Those seem to work particularly well for some Central European countries, e.g. Germany. Generally not so well for orthodox countries due to the different calendar. Additionally to the official holidays, add the days from December 24 to January 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_holidays = {}\n",
    "yrs = years.copy()\n",
    "for c in countries:\n",
    "    try:\n",
    "        list_holidays[c] = holidays.CountryHoliday(c, years = yrs)\n",
    "    except KeyError:\n",
    "        if c == 'EL':\n",
    "            list_holidays[c] = holidays.CountryHoliday('GR', years = yrs)\n",
    "        else:\n",
    "            list_holidays[c] = holidays.HolidayBase() #if no holidays are available, e.g. ME, MK        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some holidays that have shown to have lower load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "julian = ['ME', 'MK', 'RS']\n",
    "gregorian = countries.copy()\n",
    "for i in countries:\n",
    "    if i in julian:\n",
    "        gregorian.remove(i)\n",
    "\n",
    "# Add last week of the year as holidays.\n",
    "for y in list(range(2010, 2016)):\n",
    "    for i in gregorian:\n",
    "        # Add the days between Christmas Eve and New Year's Eve as holidays.\n",
    "        list_holidays[i].append(date(y,1,2))\n",
    "        list_holidays[i].append(list(pd.date_range(start = date(y, 12, 24), end = date(y, 12, 31), freq = '1D')))\n",
    "    for i in julian:\n",
    "        # Add Orthodox Christmas.\n",
    "        list_holidays[i].append(list(pd.date_range(start = date(y, 1, 6), end = date(y, 1, 8), freq = '1D')))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesise data\n",
    "## Create weekly profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frames for each country to prepare regression \n",
    "dt = {}\n",
    "for i in countries:\n",
    "    dt[i] = pd.DataFrame(d[i].loc[str(startyear):str(endyear - 1)])\n",
    "    dt[i].fillna(method = 'bfill')\n",
    "    dt[i]['temp'] = temp[i]['temp']\n",
    "    dt[i]['weekday'] = d[i]['weekday']\n",
    "    dt[i]['holiday'] = False\n",
    "\n",
    "# Add holidays\n",
    "dt_with_holidays = dt.copy()\n",
    "for i in dt_with_holidays.keys():\n",
    "    for j in dt_with_holidays[i].index:\n",
    "        if j in list_holidays[i]:\n",
    "            dt_with_holidays[i].at[j, 'holiday'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same for validation year (use the same holidays)\n",
    "\n",
    "dt_val = {}\n",
    "for i in countries:\n",
    "    dt_val[i] = pd.DataFrame(d_val[i].loc[str(val_year)])\n",
    "    dt_val[i].fillna(method = 'bfill')\n",
    "    dt_val[i]['temp'] = temp_val[i]['temp']\n",
    "    dt_val[i]['weekday'] = d_val[i]['weekday']\n",
    "    dt_val[i]['holiday'] = False\n",
    "\n",
    "dt_with_holidays_val = dt_val.copy()\n",
    "for i in dt_with_holidays_val.keys():\n",
    "    for j in dt_with_holidays_val[i].index:\n",
    "        if j in list_holidays[i]:\n",
    "            dt_with_holidays_val[i].at[j, 'holiday'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weekly profiles\n",
    "data_daily_values = dt.copy()\n",
    "for i in dt.keys():\n",
    "    data_daily_values[i] = data_daily_values[i].resample('1D').mean()\n",
    "\n",
    "# Need to manually add the last value as a copy because the last day would not be covered (as the daily values are fixed to midnight).\n",
    "\n",
    "to_be_added = {}\n",
    "for i in data_daily_values.keys():\n",
    "    to_be_added[i] = data_daily_values[i].loc[data_daily_values[i].index[-1]] #take the last value from `data_daily_values` and copy it and manually paste it into the \n",
    "    # next time step which would be in a new year\n",
    "    to_be_added[i] = pd.DataFrame(to_be_added[i]).T\n",
    "    to_be_added[i].index = pd.date_range(start = pd.Timestamp(year = endyear, month = 1, day = 1), end = pd.Timestamp(year = endyear, month = 1, day = 1), freq = '1h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary `daily_values` to store the added column additionally to our daily aggregation from `data_daily_value'.\n",
    "daily_values = {}\n",
    "for i in data_daily_values.keys():\n",
    "    daily_values[i] = data_daily_values[i].copy()\n",
    "    daily_values[i] = daily_values[i].append(to_be_added[i])\n",
    "    daily_values[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict `test_set` where we increase the frequency of the daily values and fill it with values for the day and then add this to a new dict of dataframes `joined` coming from `data_daily`.\n",
    "data_daily = dt.copy()\n",
    "test_set = {}\n",
    "joined = {}\n",
    "for i in data_daily_values.keys():\n",
    "    test_set[i] = pd.DataFrame()\n",
    "    test_set[i] = daily_values[i].asfreq('H', method='ffill')\n",
    "    val = test_set[i].loc[data_daily[i].index]['demand'].values\n",
    "    val2 = test_set[i].loc[data_daily[i].index]['temp'].values\n",
    "    dailydemand = pd.DataFrame(val, index = data_daily[i].index, columns = ['demand'])\n",
    "    joined[i] = data_daily[i].join(dailydemand, rsuffix='_daily')\n",
    "    dailytemp = pd.DataFrame(val2, index = data_daily[i].index, columns = ['temp'])\n",
    "    joined[i] = joined[i].join(dailytemp, rsuffix='_daily')                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy from `joined`, `normalised_values` where we normalise the demand values to be able to regress and obtain load profiles for each country.\n",
    "normalised_values = joined.copy()\n",
    "weekly_load = {}\n",
    "for i in normalised_values.keys():\n",
    "    normalised_values[i]['demand'] = normalised_values[i]['demand']/normalised_values[i]['demand_daily']\n",
    "    weekly_load[i] = normalised_values[i][['demand', 'weekday']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress on the normalised demand curves to obtain parameters `parameters_weekly` which we store in a dataframe `data_par_weekly`. This gives us our normalised load profile per week and country.\n",
    "\n",
    "load, aic, data_par_weekly_reordered = load_regression(place=countries, normalised_profile = weekly_load, trend = 0)\n",
    "\n",
    "# We can ignore the warning of frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in load.keys():\n",
    "# remove statistically insignificant parameters\n",
    "    for (j,x) in enumerate(load[i].params):\n",
    "        if load[i].pvalues[j] > 0.025:\n",
    "            load[i].params[j] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add heating and cooling degree days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_hc = compute_cdd_hdd(data_daily_values, countries, threshold_hdd = 15.5, threshold_cdd = 15.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for validation\n",
    "data_daily_values_val = {}\n",
    "for i in countries:\n",
    "    data_daily_values_val[i] = dt_val[i].resample('1D').mean()\n",
    "\n",
    "daily_hc_val = compute_cdd_hdd(data_daily_values_val, countries, threshold_hdd = 15.5, threshold_cdd = 15.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regress on temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BE 9 has p-value 0.10023387534657528 therefore we consider it statistically insignificant and set the parameter to 0\n",
      "CZ 0 has p-value 0.35391293002515434 therefore we consider it statistically insignificant and set the parameter to 0\n",
      "CZ 9 has p-value 0.2141135665487186 therefore we consider it statistically insignificant and set the parameter to 0\n",
      "DE 9 has p-value 0.04920921467102728 therefore we consider it statistically insignificant and set the parameter to 0\n",
      "DK 9 has p-value 0.962493941634036 therefore we consider it statistically insignificant and set the parameter to 0\n",
      "EE 0 has p-value 0.06796761603156662 therefore we consider it statistically insignificant and set the parameter to 0\n",
      "EE 9 has p-value 0.26420905649336535 therefore we consider it statistically insignificant and set the parameter to 0\n",
      "UK 9 has p-value 0.8896105925895078 therefore we consider it statistically insignificant and set the parameter to 0\n",
      "IE 0 has p-value 0.060944271076988646 therefore we consider it statistically insignificant and set the parameter to 0\n",
      "LU 9 has p-value 0.5118125736369108 therefore we consider it statistically insignificant and set the parameter to 0\n",
      "SK 0 has p-value 0.19550974685255662 therefore we consider it statistically insignificant and set the parameter to 0\n",
      "SK 9 has p-value 0.10771231093171173 therefore we consider it statistically insignificant and set the parameter to 0\n"
     ]
    }
   ],
   "source": [
    "par_hc_l, temp_hc_l, aic_hc_l, trend_hc_l, fourier_hc_l = daily_regression(place=countries,dict = dt, weekly_regression = daily_hc, daily = False, trend = 1, fourier_component = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create daily artificial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_data, days, first_day = create_daily_data(par_hc_l, trend_hc_l, daily_hc, temp_hc_l, countries, start = startyear, end = endyear, validation_days = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the regression parameters for hours of the week, the trend and temperature regression parameters\n",
    "data_par_weekly_reordered.to_csv('processing/reg_parameters_hours_of_week_2010-2014.csv')\n",
    "par_hc_l.to_csv('processing/reg_parameters_days_of_week_2010-2014.csv')\n",
    "trend_hc_l.T.to_csv('processing/reg_parameters_trend_2010-2014.csv')\n",
    "temp_hc_l.T.to_csv('processing/reg_parameters_temp_2010-2014.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create hourly artificial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_hourly = create_hourly_data(artificial_data, data_par_weekly_reordered, dt, first_day, countries, startyear, endyear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation and random mean square error.\n",
    "frequencies = ['1H', '1D']\n",
    "corr_hourly = compute_correlation(data = artificial_hourly, data_comparison = dt, column = None, place = countries, frequency = frequencies)\n",
    "rmse_hourly = compute_rmse(artificial_hourly, dt, column = None, place = countries, frequency = frequencies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create artificial data for the validation period.\n",
    "# Daily\n",
    "artificial_data_val, days_val, first_day_val = create_daily_data(par_hc_l, trend_hc_l, daily_hc_val, temp_hc_l, countries,start = val_year, end = val_year+1, validation_days = days)\n",
    "# Hourly\n",
    "artificial_hourly_val = create_hourly_data(artificial_data_val, data_par_weekly_reordered, dt_val, firstday = first_day_val, place=countries, start = val_year, end = val_year+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the Swiss data is manually added it leads to some NaN values, so we fill them with 0 for the validation)\n",
    "for i in countries:\n",
    "    dt_val[i] = dt_val[i].fillna(0)\n",
    "\n",
    "# Compute the correlation between the artificial and validation data\n",
    "corr_val = compute_correlation(data = artificial_hourly_val, \n",
    "                               data_comparison = dt_val, \n",
    "                               column = None, \n",
    "                               place = countries, \n",
    "                               frequency = frequencies)\n",
    "# Compute the RMSE for artificial and validation data\n",
    "rmse_val = compute_rmse(artificial_hourly_val, dt_val, column = None, place = countries, frequency = frequencies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the hourly average RMSE in percent.\n",
    "perc_error = {}\n",
    "for i in countries:\n",
    "    perc_error[i] = round(100 * (rmse_val.loc[i]['1H']/artificial_hourly_val[i].mean()),1)\n",
    "perc_error = pd.DataFrame.from_dict(perc_error).T\n",
    "perc_error.columns = ['hourly avg. RMSE in percent']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the validation tables for the GitHub README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = pd.concat([pd.DataFrame(corr_val['1H'].droplevel(1)),pd.DataFrame(rmse_val['1H'].droplevel(1)),perc_error],axis=1)\n",
    "validation.columns=[\"Corr\", \"RMSE\", \"Hourly avg. RMSE in %\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|                       |     AT |     BE |     BG |     CH |     CZ |      DE |     DK |    EE |      ES |     FI |      FR |      UK |     EL |     HR |     HU |     IE |      IT |    LT |    LU |    LV |    ME |    MK |     NL |      NO |     PL |     PT |     RO |     RS |     SE |    SI |     SK |\\n|-----------------------|--------|--------|--------|--------|--------|---------|--------|-------|---------|--------|---------|---------|--------|--------|--------|--------|---------|-------|-------|-------|-------|-------|--------|---------|--------|--------|--------|--------|--------|-------|--------|\\n| Corr                  |   0.97 |   0.94 |   0.88 |   0.93 |   0.96 |    0.97 |   0.95 |  0.96 |    0.95 |   0.94 |    0.95 |    0.91 |   0.89 |   0.93 |   0.95 |   0.93 |    0.94 |  0.97 |  0.74 |  0.97 |  0.91 |  0.93 |   0.94 |    0.93 |   0.96 |   0.95 |   0.92 |   0.95 |   0.95 |  0.94 |   0.94 |\\n| RMSE                  | 528.1  | 551.2  | 422.1  | 392.4  | 404.5  | 3275.2  | 298.2  | 58.1  | 1980.3  | 480.3  | 3624.7  | 3442.6  | 933.3  | 133.5  | 264.7  | 227.4  | 3070.8  | 61.2  | 94.8  | 49.5  | 38.3  | 78.8  | 775.6  | 1038.9  | 804.9  | 389.2  | 415.1  | 301.3  | 900.7  | 91.6  | 154.2  |\\n| Hourly avg. RMSE in % |   6.4  |   5.8  |  10    |   5.5  |   5.7  |    5.7  |   8.1  |  6.6  |    7.3  |   5.2  |    6.8  |    8.9  |  18.4  |   7    |   5.9  |   7.7  |    8.7  |  5    | 14.3  |  6    | 10.4  |  8.5  |   6    |    7.2  |   4.8  |   7.3  |   7.2  |   6.8  |   5.9  |  6    |   4.9  |'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabulate.tabulate(validation.T, validation.index, tablefmt=\"github\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional validation comparing to JRC-IDEES electric heating demand\n",
    "\n",
    "# Create data where heating is set to 0, i.e. obtain the electric demand that is temperature-independent + cooling demand\n",
    "no_temperature = {}\n",
    "for i in countries:\n",
    "    no_temperature[i] = daily_hc_val[i].copy(deep=True)\n",
    "    no_temperature[i]['heating'] = 0\n",
    "\n",
    "\n",
    "artificial_data_nt, days, first_day = create_daily_data(par_hc_l, trend_hc_l, no_temperature, temp_hc_l, countries, start = val_year, end = val_year+1, validation_days = days)\n",
    "\n",
    "artificial_hourly_nt = create_hourly_data(artificial_data_nt, data_par_weekly_reordered, dt_val, first_day, countries, start = val_year, end = val_year+1)\n",
    "\n",
    "#all hourly values are in MW\n",
    "heating_yearly = {}\n",
    "non_eu_countries = ['CH', 'ME', 'MK', 'NO', 'RS']\n",
    "eu_countries = [i for i in countries if (i not in non_eu_countries)]\n",
    "for i in eu_countries:\n",
    "    heating_yearly[i] = round(((artificial_hourly_val[i] - artificial_hourly_nt[i])/10**6).sum(),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paste in [JRC-IDEES heating demand](../original%20data/jrc_2015_electric_heating_demand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_heating = {'AT': 335+48,\n",
    "'BE': 251+506,\n",
    "'BG': 97+110,\n",
    "'CZ': 251+367,\n",
    "'DE': 2250+1177,\n",
    "'DK': 108+40,\n",
    "'EE': 11+85,\n",
    "'EL': 129+263,\n",
    "'ES': 939+651,\n",
    "'FI': 676+528,\n",
    "'FR': 2776+2966,\n",
    "'HR': 36+74,\n",
    "'HU': 88+57,\n",
    "'IE': 137+60,\n",
    "'IT': 381+1435,\n",
    "'LT': 19+22,\n",
    "'LU': 17+32,\n",
    "'LV': 7+37,\n",
    "'NL': 290+285,\n",
    "'PL': 196+896,\n",
    "'PT': 18+322,\n",
    "'RO': 148+27,\n",
    "'SE': 1448+590,\n",
    "'SI': 42+47,\n",
    "'SK': 80+81,\n",
    "'UK': 2791+1296}\n",
    "\n",
    "for i in space_heating.keys():\n",
    "    space_heating[i] = 1.163e-2*space_heating[i] #given data is in ktoe, so multiply it by 1.163e-2 to get TWh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare JRC electricity for space heating to artificial electric heating data:\n",
    "explained = {}\n",
    "for i in eu_countries:\n",
    "    if space_heating[i] == 0:\n",
    "        explained[i] = None\n",
    "    else:\n",
    "        explained[i] = round(heating_yearly[i]/space_heating[i],2)*100\n",
    "perc_explained = pd.DataFrame.from_dict(explained, orient = 'index', columns = ['detected electric heating demand in %']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|   AT |   BE |   BG |   CZ |   DE |   DK |   EE |   ES |   FI |   FR |   UK |   EL |   HR |   HU |   IE |   IT |   LT |   LU |   LV |   NL |   PL |   PT |   RO |   SE |   SI |   SK |\\n|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|\\n|   35 |  -31 |  112 |  -15 |  -39 |   86 |   16 |  -30 |    6 |   25 |  -31 |  -10 |    9 |   -5 |   22 |  -54 |   68 |  -65 |   56 |  -21 |  -31 |  -42 |   92 |   40 |  -42 |    7 |'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabulate.tabulate(perc_explained.values - 100, perc_explained.columns, tablefmt=\"github\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "75977c85990619d1647f36589d3e4595fb105b474683d763bf223bafd2ba259d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
